{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8fce11-1528-4a97-add3-f5ab2a47464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /opt/anaconda3/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: cftime in /opt/anaconda3/lib/python3.11/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from netCDF4) (2025.1.31)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from netCDF4) (1.26.4)\n",
      "<class 'netCDF4.Dataset'>\n",
      "root group (NETCDF3_CLASSIC data model, file format NETCDF3):\n",
      "    title: Waves-in-ice data collected during SIPEX II\n",
      "    time_coverage_start: 22/09/2012 21:00 AEST\n",
      "    time_coverage_end: 02/11/2012 18:00 AEST\n",
      "    lat_min: 63 0 S\n",
      "    lat_max: 60 30 S\n",
      "    lon_min: 121 0 E\n",
      "    lon_max: 130 0 E\n",
      "    lat and lon units: degrees minutes\n",
      "    institution: NZ's National Institute for Water and Atmospheric Research and the Australian Antarctic Division\n",
      "    creator_name: Alison Kohout\n",
      "    creator_email: alison.kohout@niwa.co.nz\n",
      "    project: Sea Ice and Physics Experiment II (SIPEXII)\n",
      "    contributor_name: Alison Kohout, Mike Williams, Bill Penrose, Scott Penrose, Klaus Meiners, Andy Cianchi,Murray Doyle, Leigh Hornsby, Jan Lieser, Takenobu Toyota, Christian Gallagher\n",
      "    contributor_role: project leader, project supervisor, electrical engineer, software engineer,SIPEX II chief investigator, voyage leader, voyage captain, helicopter captain,helicopter support, science support, field support\n",
      "    summary: Our aim during SIPEXII was to observe waves in the marginal ice zone (MIZ).  Eight custom made wave sensors were built for the task. The sensors were deployed in the Antarctic MIZ along a north - south transect spread over 200 km. Every three hours, the sensors simultaneously woke and recorded a burst of wave acceleration data. Each sensor performed on-board data quality control and spectral analysis.The wave spectrum was returned via satellite. Ship based operations were funded through the Australian Antarctic Division (AAD). NZ’s National Institute for Water and Atmospheric Research (NIWA) funded the design and building of the instruments. The preparation and research pre voyage was funded by NZ’s Foundation for research, science and technology and the analysis will be funded by NZ’s Marsden fund a NIWA post\n",
      "    date_created: 06/03/2012\n",
      "    date_modified: 06/03/2012\n",
      "    reference: to be announced\n",
      "    keywords: waves, sea-ice, antarctica, marginal ice zone, sipexII\n",
      "    history: Mon Sep 23 08:04:55 2013: ncrcat -n 3,2,1 waves-in-ice_2012-09.nc ../waves-in-ice.nc\n",
      "Mon Sep 23 08:04:54 2013: ncrcat -n 8,2,1 waves-in-ice_2012-09-23.nc ../monthly/waves-in-ice_2012-09.nc\n",
      "Mon Sep 23 08:04:53 2013: ncrcat -n 8,6,30000 waves-in-ice_2012-09-23_000000.nc ../daily/waves-in-ice_2012-09-23.nc\n",
      "    nco_openmp_thread_number: 1\n",
      "    dimensions(sizes): time(325), sensor(8), bin(55), moments_header(6), orientation_header(3), direction_header(2), sd_gyro_header(3), time_header(100), chid(40)\n",
      "    variables(dimensions): int32 time(time), float64 sensor(sensor), float64 bin(bin), |S1 moments_header(moments_header, chid), |S1 orientation_header(orientation_header, chid), |S1 direction_header(direction_header, chid), |S1 sd_gyro_header(sd_gyro_header, chid), |S1 time_header(time_header, chid), float64 psd(time, sensor, bin), float64 moments(time, sensor, moments_header), float64 direction(time, sensor, direction_header), float64 dir_ratio(time, sensor), float64 orientation(time, sensor, orientation_header), float64 sd_gyro(time, sensor, sd_gyro_header), float64 lat(time, sensor), float64 lon(time, sensor), int32 temp(time, sensor), int32 volts(time, sensor), int32 elev(time, sensor), float64 sd_acc(time, sensor), float64 sd_yaw(time, sensor), float64 acc_removed(time, sensor), float64 flat(time, sensor), int32 max_flat(time, sensor), int32 spike(time, sensor), int32 max_spike(time, sensor), int32 acc_flag(time, sensor), int32 imu_flag(time, sensor), int32 power_flag(time, sensor), |S1 file_id(time, sensor), |S1 time_str(time, time_header), |S1 time_str_utc(time, time_header)\n",
      "    groups: \n",
      "dict_keys(['time', 'sensor', 'bin', 'moments_header', 'orientation_header', 'direction_header', 'sd_gyro_header', 'time_header', 'psd', 'moments', 'direction', 'dir_ratio', 'orientation', 'sd_gyro', 'lat', 'lon', 'temp', 'volts', 'elev', 'sd_acc', 'sd_yaw', 'acc_removed', 'flat', 'max_flat', 'spike', 'max_spike', 'acc_flag', 'imu_flag', 'power_flag', 'file_id', 'time_str', 'time_str_utc'])\n"
     ]
    }
   ],
   "source": [
    "!pip install netCDF4\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "dataset = Dataset('sipexII.nc')\n",
    "print(dataset)  # Prints summary info\n",
    "print(dataset.variables.keys())  # Prints variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a75a2ebb-1c51-456c-bdc5-998b583b10f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions available: dict_keys(['time', 'sensor', 'bin', 'moments_header', 'orientation_header', 'direction_header', 'sd_gyro_header', 'time_header', 'chid'])\n",
      "Dimension 'sensor': size = 8\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "# Open the NetCDF file\n",
    "dataset = Dataset('sipexII.nc')\n",
    "\n",
    "# List all dimension names\n",
    "print(\"Dimensions available:\", dataset.dimensions.keys())\n",
    "\n",
    "# Access a specific dimension by name, e.g. 'time'\n",
    "dim_name = 'sensor'  \n",
    "if dim_name in dataset.dimensions:\n",
    "    dimension = dataset.dimensions[dim_name]\n",
    "    print(f\"Dimension '{dim_name}': size = {len(dimension)}\")\n",
    "else:\n",
    "    print(f\"Dimension '{dim_name}' not found in the file.\")\n",
    "\n",
    "# Close the dataset\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0e3baa-2a0f-43c5-8266-dcdc7afe537f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables available: dict_keys(['time', 'sensor', 'bin', 'moments_header', 'orientation_header', 'direction_header', 'sd_gyro_header', 'time_header', 'psd', 'moments', 'direction', 'dir_ratio', 'orientation', 'sd_gyro', 'lat', 'lon', 'temp', 'volts', 'elev', 'sd_acc', 'sd_yaw', 'acc_removed', 'flat', 'max_flat', 'spike', 'max_spike', 'acc_flag', 'imu_flag', 'power_flag', 'file_id', 'time_str', 'time_str_utc'])\n",
      "Variable 'lon':\n",
      "<class 'netCDF4.Variable'>\n",
      "float64 lon(time, sensor)\n",
      "    standard_name: longitude\n",
      "    units: decimal degrees\n",
      "unlimited dimensions: time\n",
      "current shape = (325, 8)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used\n",
      "[120.87727 121.4132 121.15723 121.18118 121.08153 120.99311 121.08929\n",
      " 121.24235 121.33202 121.38866 121.52528 121.68403 121.77295 121.77548\n",
      " 121.79457 121.87272 121.92278 121.89477 121.86188 121.90566 121.95464\n",
      " 121.95838 121.93791 121.93616 121.9595 121.98741 121.96163 121.91685\n",
      " 121.91842 121.96014 121.95726 121.92022 121.89379 121.93831 122.00356\n",
      " 122.00247 121.95775 121.9735 122.03023 122.04804 122.05338 122.10619\n",
      " 122.19435 122.26508 122.30359 122.34806 122.40126 122.49987 122.62144\n",
      " 122.71454 122.79964 122.92363 123.06761 123.19399 123.28522 123.34666\n",
      " 123.43225 123.53324 123.61554 123.6528 123.69258 123.73345 123.73509\n",
      " 123.68836 123.63833 123.65904 123.7388 123.82354 123.89495 124.00033\n",
      " 124.15428 124.25918 124.36027 124.50277 124.6357 124.79879 124.94882\n",
      " 125.14312 125.33914 125.53785 125.66371 125.85056 125.90656 125.9099\n",
      " 125.88725 125.91209 125.99026 126.0607 126.12653 126.21677 126.29897\n",
      " 126.3884 126.44614 126.49639 126.5726 126.65802 126.70995 126.74815\n",
      " 126.85329 126.99821 127.09878 127.13698 127.16612 127.23375 127.33709\n",
      " 127.41904 127.48065 127.55977 127.66491 127.7449 127.78761 127.82534\n",
      " 127.87328 127.96631 128.12316 128.22771 128.30419 128.39149 128.48425\n",
      " 128.53714 128.54683 128.60881 128.74188 128.86245 128.9029 128.91679\n",
      " 128.98071 129.06753 129.16388 129.21349 129.25437 129.35175 129.45864\n",
      " 129.54645 129.64244 129.73842 -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- --]\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "# Open the NetCDF file\n",
    "dataset = Dataset('sipexII.nc')\n",
    "\n",
    "# List all variable names\n",
    "print(\"Variables available:\", dataset.variables.keys())\n",
    "\n",
    "# Access a specific variable by name, e.g. 'temperature'\n",
    "var_name = 'lon'  # Replace with your actual variable name\n",
    "if var_name in dataset.variables:\n",
    "    variable = dataset.variables[var_name]\n",
    "    print(f\"Variable '{var_name}':\")\n",
    "    print(variable)\n",
    "    # Print variable data (e.g. first 10 elements if it's an array)\n",
    "    print(variable[1:1000,2])  \n",
    "else:\n",
    "    print(f\"Variable '{var_name}' not found in the file.\")\n",
    "\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "def64b6d-dd68-423c-82fe-a059eb3d1cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices with longitude ~ 121.17186: [4]\n",
      "Sensor IDs with target longitude: [5.]\n",
      "Latitudes of sensors with target longitude: [[-- -- -61.51758 -61.41227 -61.57818 -61.44243 -- --]]\n"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Open the NetCDF file\n",
    "dataset = Dataset('sipexII.nc')\n",
    "\n",
    "# Load longitude variable (replace 'lon' with your longitude variable name)\n",
    "lon = dataset.variables['lon'][:]  # e.g., shape could be (num_sensors,)\n",
    "\n",
    "# Find indices where longitude equals 121.13549 (with some tolerance for floating point)\n",
    "target_lon = 121.17186\n",
    "tolerance = 1e-7  # adjust tolerance as needed\n",
    "indices = np.where(np.abs(lon - target_lon) < tolerance)[0]\n",
    "\n",
    "print(f\"Indices with longitude ~ {target_lon}: {indices}\")\n",
    "\n",
    "# If you want to print some other info for those sensors, e.g., sensor IDs or latitudes\n",
    "# Replace 'sensor_id' and 'lat' with your variable names, if available\n",
    "if 'sensor' in dataset.variables:\n",
    "    sensor_ids = dataset.variables['sensor'][:]\n",
    "    print(\"Sensor IDs with target longitude:\", sensor_ids[indices])\n",
    "if 'lat' in dataset.variables:\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    print(\"Latitudes of sensors with target longitude:\", lat[indices])\n",
    "\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b50f7bec-324c-4965-b588-5b7a057c5ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- --]\n"
     ]
    }
   ],
   "source": [
    "print(lon[0:1000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e8ed9422-953f-4297-b559-fe60ad3cd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file generated: sensor_1_data.csv\n",
      "CSV file generated: sensor_2_data.csv\n",
      "CSV file generated: sensor_3_data.csv\n",
      "CSV file generated: sensor_4_data.csv\n",
      "CSV file generated: sensor_5_data.csv\n",
      "CSV file generated: sensor_6_data.csv\n",
      "CSV file generated: sensor_7_data.csv\n",
      "CSV file generated: sensor_8_data.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "nc_file = 'sipexII.nc'\n",
    "sensor_id_to_extract = 9\n",
    "\n",
    "dataset = Dataset(nc_file)\n",
    "\n",
    "# Read variables - replace names as needed\n",
    "time = dataset.variables['time'][:]          # shape (time,)\n",
    "lat = dataset.variables['lat'][:]            # shape (sensor,)\n",
    "lon = dataset.variables['lon'][:]            # shape (sensor,)\n",
    "sensor_ids = dataset.variables['sensor'][:]  # assuming 1D sensor list\n",
    "moments = dataset.variables['moments'][:]    # e.g. shape (time, sensor, ...)\n",
    "bin_var = dataset.variables['bin'][:]        # shape and dims depend on data\n",
    "psd = dataset.variables['psd'][:] # e.g. shape (time, sensor, ...)\n",
    "moments_header = dataset.variables['moments_header'][:]\n",
    "\n",
    "def process_sensor(sensor_id):\n",
    "\n",
    "    # Find index of sensor\n",
    "    sensor_indices = np.where(sensor_ids == sensor_id)[0]\n",
    "    if len(sensor_indices) == 0:\n",
    "        raise ValueError(f\"Sensor {sensor_id_to_extract} not found\")\n",
    "    sensor_idx = sensor_indices[0]\n",
    "\n",
    "    # Because time series: iterate over time dimension\n",
    "    rows = []\n",
    "    for t_idx, t in enumerate(time):\n",
    "        # Extract values for sensor 1 at time t\n",
    "        lat_val = lat[t_idx,sensor_idx]\n",
    "        lon_val = lon[t_idx,sensor_idx]\n",
    "        moment_val = moments[t_idx, sensor_idx,:] \n",
    "        bin_val = bin_var \n",
    "        psd_val = psd[t_idx, sensor_idx,:]\n",
    "\n",
    "        # Check for fill/default/NaN values\n",
    "        def is_valid(x):\n",
    "            if isinstance(x, np.ndarray):\n",
    "                # If array, check all values (or define rules to check)\n",
    "                return not (np.any(np.isnan(x)) )\n",
    "            else:\n",
    "                return (not np.isnan(x)) \n",
    "\n",
    "        if (is_valid(t) and\n",
    "            is_valid(lat_val) and\n",
    "            is_valid(lon_val) and\n",
    "            is_valid(moment_val) and\n",
    "            is_valid(bin_val) and\n",
    "            is_valid(psd_val)):\n",
    "            rows.append({\n",
    "                'time': t,\n",
    "                'lat': lat_val,\n",
    "                'lon': lon_val,\n",
    "                'moments': moment_val if np.isscalar(moment_val) else moment_val.tolist(),\n",
    "                'bin': bin_val if np.isscalar(bin_val) else bin_val.tolist(),\n",
    "                'psd': psd_val if np.isscalar(psd_val) else psd_val.tolist(),\n",
    "            })\n",
    "\n",
    "    # Create DataFrame from rows\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_filename = f'sensor_{sensor_id}_data.csv'\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"CSV file generated: {csv_filename}\")\n",
    "\n",
    "# Sensors to process: 1, 8\n",
    "sensor_list = range(1,11)\n",
    "sensor_list = sorted(set(sensor_list))  # Remove duplicates if any\n",
    "\n",
    "for sid in sensor_list:\n",
    "    process_sensor(sid)\n",
    "\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd50cc-f417-47fa-bb8d-1de041879661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
