{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8fce11-1528-4a97-add3-f5ab2a47464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netCDF4\n",
    "#from netCDF4 import Dataset\n",
    "\n",
    "#dataset = Dataset('sipexII.nc')\n",
    "#print(dataset)  # Prints summary info\n",
    "#print(dataset.variables.keys())  # Prints variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a75a2ebb-1c51-456c-bdc5-998b583b10f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file: sipexII.nc\n",
      "Variables: ['time', 'sensor', 'bin', 'moments_header', 'orientation_header', 'direction_header', 'sd_gyro_header', 'time_header', 'psd', 'moments', 'direction', 'dir_ratio', 'orientation', 'sd_gyro', 'lat', 'lon', 'temp', 'volts', 'elev', 'sd_acc', 'sd_yaw', 'acc_removed', 'flat', 'max_flat', 'spike', 'max_spike', 'acc_flag', 'imu_flag', 'power_flag', 'file_id', 'time_str', 'time_str_utc']\n",
      "Dimensions: ['time', 'sensor', 'bin', 'moments_header', 'orientation_header', 'direction_header', 'sd_gyro_header', 'time_header', 'chid']\n",
      "time_str_utc shape: (325, 100)\n",
      "time_str_utc dtype: |S1\n",
      "Sample raw time_str_utc data (first row):\n",
      "[b'2' b'2' b'/' b'0' b'9' b'/' b'2' b'0' b'1' b'2' b' ' b'1' b'4' b':'\n",
      " b'0' b'0' -- -- -- --]\n",
      "Converted time_str shape: (325,)\n",
      "Sample times: ['22/09/2012 14:00' '22/09/2012 17:00' '22/09/2012 20:00'\n",
      " '22/09/2012 23:00' '23/09/2012 2:00']\n",
      "Shapes - psd: (325, 5, 55), moments: (325, 5, 6), lat: (325, 5), lon: (325, 5)\n",
      "Found 997 data points with Hs > 20m. Setting these to NaN.\n",
      "Saved SIPEX3_data.csv with 131 rows\n",
      "Sample date strings: ['22/09/2012 14:00', '22/09/2012 14:00', '22/09/2012 14:00', '22/09/2012 14:00', '22/09/2012 14:00']\n",
      "Plot saved for sensor 3 with 131 data points\n",
      "Saved SIPEX4_data.csv with 70 rows\n",
      "Sample date strings: ['22/09/2012 17:00', '22/09/2012 17:00', '22/09/2012 17:00', '22/09/2012 17:00', '22/09/2012 17:00']\n",
      "Plot saved for sensor 4 with 70 data points\n",
      "Saved SIPEX5_data.csv with 72 rows\n",
      "Sample date strings: ['22/09/2012 20:00', '22/09/2012 20:00', '22/09/2012 20:00', '22/09/2012 20:00', '22/09/2012 20:00']\n",
      "Plot saved for sensor 5 with 72 data points\n",
      "Saved SIPEX6_data.csv with 4 rows\n",
      "Sample date strings: ['22/09/2012 23:00', '22/09/2012 23:00', '22/09/2012 23:00', '22/09/2012 23:00']\n",
      "Plot saved for sensor 6 with 4 data points\n",
      "Saved SIPEX7_data.csv with 312 rows\n",
      "Sample date strings: ['23/09/2012 2:00', '23/09/2012 2:00', '23/09/2012 2:00', '23/09/2012 2:00', '23/09/2012 2:00']\n",
      "Plot saved for sensor 7 with 312 data points\n",
      "Data processing completed.\n",
      "Script completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def process_wave_data(file_path=\"sipexII.nc\"):\n",
    "    try:\n",
    "        # Open the netCDF file\n",
    "        print(f\"Opening file: {file_path}\")\n",
    "        nc = Dataset(file_path, 'r')\n",
    "        \n",
    "        # Print variables and dimensions for debugging\n",
    "        print(\"Variables:\", list(nc.variables.keys()))\n",
    "        print(\"Dimensions:\", list(nc.dimensions.keys()))\n",
    "        \n",
    "        # Extract time string using a simpler approach\n",
    "        if 'time_str_utc' in nc.variables:\n",
    "            time_chars = nc.variables['time_str_utc'][:]\n",
    "            print(f\"time_str_utc shape: {time_chars.shape}\")\n",
    "            print(f\"time_str_utc dtype: {time_chars.dtype}\")\n",
    "            \n",
    "            # Print a sample raw data for debugging\n",
    "            print(\"Sample raw time_str_utc data (first row):\")\n",
    "            print(time_chars[0, :20])  # First 20 chars of first row\n",
    "            \n",
    "            # Convert character array to strings - approach for bytes\n",
    "            time_strings = []\n",
    "            for i in range(time_chars.shape[0]):\n",
    "                # For bytes objects\n",
    "                if time_chars.dtype.kind == 'S':  # String/bytes type\n",
    "                    # Join all bytes together and decode\n",
    "                    try:\n",
    "                        # Try to decode the whole array at once\n",
    "                        full_str = b''.join(filter(None, time_chars[i])).decode('utf-8', errors='replace').strip()\n",
    "                        time_strings.append(full_str)\n",
    "                    except:\n",
    "                        # Fallback: try to decode each byte individually\n",
    "                        chars = []\n",
    "                        for c in time_chars[i]:\n",
    "                            if c:  # Skip empty bytes\n",
    "                                try:\n",
    "                                    decoded = c.decode('utf-8', errors='replace')\n",
    "                                    chars.append(decoded)\n",
    "                                except:\n",
    "                                    pass\n",
    "                        time_strings.append(''.join(chars).strip())\n",
    "                else:\n",
    "                    # For numeric arrays (like uint8)\n",
    "                    chars = []\n",
    "                    for c in time_chars[i]:\n",
    "                        if c > 0:  # Skip nulls\n",
    "                            chars.append(chr(c))\n",
    "                    time_strings.append(''.join(chars).strip())\n",
    "            \n",
    "            time_str = np.array(time_strings)\n",
    "            print(f\"Converted time_str shape: {time_str.shape}\")\n",
    "            print(f\"Sample times: {time_str[:5]}\")\n",
    "        else:\n",
    "            time_str = None\n",
    "            print(\"Warning: 'time_str_utc' variable not found in the file\")\n",
    "        \n",
    "        # Extract other variables\n",
    "        psd = nc.variables['psd'][:, 2:7, :]  # Select sensors 2-6\n",
    "        bin_vals = nc.variables['bin'][:]\n",
    "        time = nc.variables['time'][:]\n",
    "        moments = nc.variables['moments'][:, 2:7, :]\n",
    "        sensor = nc.variables['sensor'][2:7]\n",
    "        lat = nc.variables['lat'][:, 2:7]\n",
    "        lon = nc.variables['lon'][:, 2:7]\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        print(f\"Shapes - psd: {psd.shape}, moments: {moments.shape}, lat: {lat.shape}, lon: {lon.shape}\")\n",
    "        \n",
    "        # Calculate derived values\n",
    "        T = 1/bin_vals\n",
    "        \n",
    "        # Create numpy arrays to work with (not masked arrays)\n",
    "        psd_arr = np.array(psd)\n",
    "        moments_arr = np.array(moments)\n",
    "        lat_arr = np.array(lat)\n",
    "        lon_arr = np.array(lon)\n",
    "        \n",
    "        # Use NaN as fill value for missing data\n",
    "        fill_value = np.nan\n",
    "        \n",
    "        # Function to set values to NaN\n",
    "        def set_nan(arr, indices):\n",
    "            arr_copy = arr.copy()\n",
    "            if isinstance(indices, tuple):\n",
    "                arr_copy[indices] = fill_value\n",
    "            else:\n",
    "                for idx in indices:\n",
    "                    arr_copy[idx] = fill_value\n",
    "            return arr_copy\n",
    "\n",
    "        # Remove data whilst transmitting from ship\n",
    "        lon_arr = set_nan(lon_arr, (slice(0, 4), 0))\n",
    "        lat_arr = set_nan(lat_arr, (slice(0, 4), 0))\n",
    "        lon_arr = set_nan(lon_arr, (slice(0, 4), 1))\n",
    "        lat_arr = set_nan(lat_arr, (slice(0, 4), 1))\n",
    "        lon_arr = set_nan(lon_arr, (slice(0, 6), 2))\n",
    "        lat_arr = set_nan(lat_arr, (slice(0, 6), 2))\n",
    "        lon_arr = set_nan(lon_arr, (slice(0, 4), 3))\n",
    "        lat_arr = set_nan(lat_arr, (slice(0, 4), 3))\n",
    "        lon_arr = set_nan(lon_arr, (slice(0, 12), 4))\n",
    "        lat_arr = set_nan(lat_arr, (slice(0, 12), 4))\n",
    "        \n",
    "        psd_arr = set_nan(psd_arr, (slice(0, 4), 0, slice(None)))\n",
    "        psd_arr = set_nan(psd_arr, (slice(0, 4), 1, slice(None)))\n",
    "        psd_arr = set_nan(psd_arr, (slice(0, 6), 2, slice(None)))\n",
    "        psd_arr = set_nan(psd_arr, (slice(0, 4), 3, slice(None)))\n",
    "        psd_arr = set_nan(psd_arr, (slice(0, 12), 4, slice(None)))\n",
    "        \n",
    "        moments_arr = set_nan(moments_arr, (slice(0, 4), 0, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (slice(0, 4), 1, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (slice(0, 6), 2, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (slice(0, 4), 3, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (slice(0, 12), 4, slice(None)))\n",
    "        \n",
    "        # Fill bad values with missing values\n",
    "        # Spikes in sensor 0\n",
    "        psd_arr = set_nan(psd_arr, (85, 0, slice(None)))\n",
    "        psd_arr = set_nan(psd_arr, (100, 0, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (85, 0, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (100, 0, slice(None)))\n",
    "        lat_arr = set_nan(lat_arr, (85, 0))\n",
    "        lat_arr = set_nan(lat_arr, (100, 0))\n",
    "        \n",
    "        # Spikes in sensor 1\n",
    "        psd_arr = set_nan(psd_arr, (42, 1, slice(None)))\n",
    "        psd_arr = set_nan(psd_arr, (slice(73, 75), 1, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (42, 1, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (slice(73, 75), 1, slice(None)))\n",
    "        lat_arr = set_nan(lat_arr, (42, 1))\n",
    "        lat_arr = set_nan(lat_arr, (slice(73, 75), 1))\n",
    "        \n",
    "        # Gyro issues in sensor 3\n",
    "        psd_arr = set_nan(psd_arr, (slice(8, 11), 3, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (slice(8, 11), 3, slice(None)))\n",
    "        lat_arr = set_nan(lat_arr, (slice(8, 11), 3))\n",
    "        \n",
    "        # Spikes in sensor 4\n",
    "        psd_arr = set_nan(psd_arr, (318, 4, slice(None)))\n",
    "        moments_arr = set_nan(moments_arr, (318, 4, slice(None)))\n",
    "        lat_arr = set_nan(lat_arr, (318, 4))\n",
    "        \n",
    "        # Calculate Significant Wave Height (Hs)\n",
    "        num_times = psd_arr.shape[0]\n",
    "        Hs = np.full((num_times, 5), fill_value)\n",
    "        for i in range(5):\n",
    "            for j in range(num_times):\n",
    "                if not np.isnan(moments_arr[j, i, 2]):\n",
    "                    Hs[j, i] = 4 * np.sqrt(moments_arr[j, i, 2])\n",
    "                    \n",
    "        # Filter out unrealistic Hs values (> 20m)\n",
    "        unrealistic_mask = Hs > 20.0\n",
    "        if np.any(unrealistic_mask):\n",
    "            unrealistic_count = np.sum(unrealistic_mask)\n",
    "            print(f\"Found {unrealistic_count} data points with Hs > 20m. Setting these to NaN.\")\n",
    "            \n",
    "            # Set Hs to NaN where values are unrealistic\n",
    "            Hs[unrealistic_mask] = fill_value\n",
    "            \n",
    "            # Also set corresponding Tp values to NaN (will calculate later)\n",
    "            # and lat/lon values\n",
    "            for i in range(5):\n",
    "                for j in range(num_times):\n",
    "                    if unrealistic_mask[j, i]:\n",
    "                        # Set lat and lon to NaN for the same indices\n",
    "                        lat_arr[j, i] = fill_value\n",
    "                        lon_arr[j, i] = fill_value\n",
    "                        # We'll set Tp to NaN when we calculate it\n",
    "        \n",
    "        # No waves condition\n",
    "        Hs[41:43, 4] = 0\n",
    "        \n",
    "        # Calculate Peak Period (Tp)\n",
    "        Tp = np.full((num_times, 5), fill_value)\n",
    "        for i in range(5):\n",
    "            for j in range(num_times):\n",
    "                # Skip if Hs is NaN (unrealistic values) or all psd values are NaN\n",
    "                if np.isnan(Hs[j, i]) or np.all(np.isnan(psd_arr[j, i, :])):\n",
    "                    continue\n",
    "                    \n",
    "                # Find index of max value, ignoring NaNs\n",
    "                valid_indices = ~np.isnan(psd_arr[j, i, :])\n",
    "                if np.any(valid_indices):\n",
    "                    max_idx = np.nanargmax(psd_arr[j, i, :])\n",
    "                    Tp[j, i] = T[max_idx]\n",
    "                \n",
    "        # Process each sensor and create CSV files\n",
    "        for sensor_idx in range(5):\n",
    "                    \n",
    "            # Original sensor number (adding 2 because we selected 2:7)\n",
    "            original_sensor_id = sensor[sensor_idx]\n",
    "            \n",
    "            # Create data for DataFrame\n",
    "            data = {\n",
    "                'DD/MM/YYYY HH:MM (UTC)': time_str[:],\n",
    "                'Latitude (decimal degrees)': lat_arr[:, sensor_idx],\n",
    "                'Longitude (decimal degrees)': lon_arr[:, sensor_idx],\n",
    "                'Significant Wave Height (m)': Hs[:, sensor_idx],\n",
    "                'Peak Period (s)': Tp[:, sensor_idx]\n",
    "            }\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Filter out rows with missing lat/lon\n",
    "            df_filtered = df.dropna(subset=['Latitude (decimal degrees)', 'Longitude (decimal degrees)'])\n",
    "            \n",
    "            # Save to CSV\n",
    "            csv_filename = f'SIPEX{int(original_sensor_id)}_data.csv'\n",
    "            df_filtered.to_csv(csv_filename, index=False)\n",
    "            print(f\"Saved {csv_filename} with {len(df_filtered)} rows\")\n",
    "            \n",
    "            # Create a plot for verification\n",
    "            try:\n",
    "                # Filter data to include only non-null values for both wave height and period\n",
    "                plot_df = df_filtered.dropna(subset=['Significant Wave Height (m)', 'Peak Period (s)', 'DD/MM/YYYY HH:MM (UTC)'])\n",
    "                \n",
    "                if len(plot_df) == 0:\n",
    "                    print(f\"No valid data to plot for sensor {int(original_sensor_id)}\")\n",
    "                    return\n",
    "                \n",
    "                # Print sample of date strings to debug\n",
    "                print(\"Sample date strings:\", plot_df['DD/MM/YYYY HH:MM (UTC)'].iloc[:5].tolist())\n",
    "                \n",
    "                # Cap Hs values to reasonable range (0-20m)\n",
    "                plot_df['Significant Wave Height (m)'] = plot_df['Significant Wave Height (m)'].clip(upper=20.0)\n",
    "                \n",
    "                # Try simpler approach: just use the row index for x-axis\n",
    "                # Create figure and primary axis for wave height\n",
    "                fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "                \n",
    "                # Plot wave height on the primary axis\n",
    "                color1 = 'tab:blue'\n",
    "                ax1.set_ylabel('Significant Wave Height (m)', color=color1, fontsize=12)\n",
    "                line1 = ax1.plot(range(len(plot_df)), plot_df['Significant Wave Height (m)'], \n",
    "                         color=color1, label='Significant Wave Height', marker='o', markersize=4)\n",
    "                ax1.tick_params(axis='y', labelcolor=color1)\n",
    "                \n",
    "                # Set y-axis limit for wave height\n",
    "                ax1.set_ylim(0, 20)\n",
    "                \n",
    "                # Create a second y-axis that shares the same x-axis\n",
    "                ax2 = ax1.twinx()\n",
    "                \n",
    "                # Plot peak period on the secondary axis\n",
    "                color2 = 'tab:red'\n",
    "                ax2.set_ylabel('Peak Period (s)', color=color2, fontsize=12)\n",
    "                line2 = ax2.plot(range(len(plot_df)), plot_df['Peak Period (s)'], \n",
    "                         color=color2, label='Peak Period', marker='x', markersize=4)\n",
    "                ax2.tick_params(axis='y', labelcolor=color2)\n",
    "                \n",
    "                # Format x-axis with date strings at select positions\n",
    "                if len(plot_df) > 20:\n",
    "                    # Select a subset of indices for x-ticks\n",
    "                    n_ticks = min(15, len(plot_df))\n",
    "                    step = max(1, len(plot_df) // n_ticks)\n",
    "                    tick_positions = list(range(0, len(plot_df), step))\n",
    "                    \n",
    "                    # Make sure the last position is included\n",
    "                    if len(plot_df) - 1 not in tick_positions:\n",
    "                        tick_positions.append(len(plot_df) - 1)\n",
    "                        \n",
    "                    # Set tick positions and labels\n",
    "                    ax1.set_xticks(tick_positions)\n",
    "                    ax1.set_xticklabels([plot_df['DD/MM/YYYY HH:MM (UTC)'].iloc[i] for i in tick_positions], rotation=45, ha='right')\n",
    "                else:\n",
    "                    # If few points, show all date labels\n",
    "                    ax1.set_xticks(range(len(plot_df)))\n",
    "                    ax1.set_xticklabels(plot_df['DD/MM/YYYY HH:MM (UTC)'], rotation=45, ha='right')\n",
    "                \n",
    "                # Add a title\n",
    "                plt.title(f'Wave Data - Sensor {int(original_sensor_id)}', fontsize=14)\n",
    "                \n",
    "                # Add legend for both lines\n",
    "                ax1.legend(line1 + line2, ['Significant Wave Height', 'Peak Period'], \n",
    "                           loc='upper right', fontsize=11)\n",
    "                \n",
    "                # Add grid for better readability\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add x-axis label\n",
    "                ax1.set_xlabel('Date/Time (UTC)', fontsize=12)\n",
    "                \n",
    "                # Adjust layout to make room for labels\n",
    "                fig.tight_layout()\n",
    "                \n",
    "                # Save figure\n",
    "                plt.savefig(f'SIPEX{int(original_sensor_id)}_plot.png', dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"Plot saved for sensor {int(original_sensor_id)} with {len(plot_df)} data points\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating plot: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Close the netCDF file\n",
    "        nc.close()\n",
    "        print(\"Data processing completed.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Call the function to process the data\n",
    "if __name__ == \"__main__\":\n",
    "    success = process_wave_data()\n",
    "    \n",
    "    if success:\n",
    "        print(\"Script completed successfully.\")\n",
    "    else:\n",
    "        print(\"Script failed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e3baa-2a0f-43c5-8266-dcdc7afe537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def64b6d-dd68-423c-82fe-a059eb3d1cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f7bec-324c-4965-b588-5b7a057c5ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed9422-953f-4297-b559-fe60ad3cd9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd50cc-f417-47fa-bb8d-1de041879661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9318c-f6e2-4e20-8919-4890b597b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
