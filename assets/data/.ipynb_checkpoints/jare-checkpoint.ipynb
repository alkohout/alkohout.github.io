{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac8fce11-1528-4a97-add3-f5ab2a47464a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4.Dataset'>\n",
      "root group (NETCDF3_CLASSIC data model, file format NETCDF3):\n",
      "    title: Waves-in-ice data collected during JARE61\n",
      "    lat and lon units: Decimal degrees\n",
      "    institution: University of Tasmania and New Zealand's National Institute for Wat\n",
      "    creator_name: Alison Kohout\n",
      "    creator_email: alison.kohout@niwa.co.nz\n",
      "    project: Australian Research Council Discovery Project DP170103774 'Advancing Wave-Ice Models with Autonomous Observations\n",
      "    contributor_name: Guy Williams, Alison Kohout, Pat Wongpan, Bill Penrose, Scott Penrose\n",
      "    contributor_role: GW: Lead Chief Investigator, AK: Chief Investigator, PW: Field Leader, BP: Engineer, SP: Software Engineer\n",
      "    summary: The aim of this project was to observe wave propagation in the marginal ice zone (MIZ). Each sensor performed on-board spectral analysis and data quality control. The data was returned via Iridium data packets. The instrumentation development, construction and deployment was funded through the Australian Research Council Discovery Project DP170103774. The science development and analysis was funded through New Zealand's National Institute of Water and Atmospheric Research Core Funding. The wave buoys were built by P.A.S consultants, Melbourne, Australia.\n",
      "    date_created: 11/04/2020\n",
      "    date_modified: 30/04/2021\n",
      "    reference: to be announced\n",
      "    keywords: waves, sea-ice, antarctica, marginal ice zone\n",
      "    dimensions(sizes): bouy(5), time(20448), moments(7), bin(55), chid(40), time_string(100)\n",
      "    variables(dimensions): int32 buoy(bouy), int32 time(time), |S1 moments_header(chid, moments), float64 bin(bin), |S1 time_string(time_string, chid), |S1 time_str(bouy, time, time_string), float64 lat(bouy, time), float64 lon(bouy, time), float64 Tp(bouy, time), float64 Hs(bouy, time), float64 Hs_processed(bouy, time), float64 moments_raw(bouy, time, moments), float64 moments_processed(bouy, time, moments), float64 psd(bouy, time, bin), float64 psd_processed(bouy, time, bin), float64 temp(bouy, time), int32 deployed(bouy, time), float64 qf_mean_removed(bouy, time), int32 qf_imu_accel(bouy, time), int32 qf_imu_gyro(bouy, time), int32 qf_imu_mag(bouy, time), int32 qf_gps(bouy, time)\n",
      "    groups: \n",
      "dict_keys(['buoy', 'time', 'moments_header', 'bin', 'time_string', 'time_str', 'lat', 'lon', 'Tp', 'Hs', 'Hs_processed', 'moments_raw', 'moments_processed', 'psd', 'psd_processed', 'temp', 'deployed', 'qf_mean_removed', 'qf_imu_accel', 'qf_imu_gyro', 'qf_imu_mag', 'qf_gps'])\n",
      "<class 'netCDF4.Variable'>\n",
      "int32 time(time)\n",
      "    standard_name: time\n",
      "    units: seconds since 2019-01-01 00:00 UTC\n",
      "    axis: T\n",
      "    calendar: gregorian\n",
      "unlimited dimensions: \n",
      "current shape = (20448,)\n",
      "filling on, default _FillValue of -2147483647 used\n",
      "Saved JARE1_data.csv with 475 rows\n",
      "Plot saved for sensor 1 with 466 data points\n",
      "Saved JARE2_data.csv with 1128 rows\n",
      "Plot saved for sensor 2 with 1058 data points\n",
      "Saved JARE3_data.csv with 636 rows\n",
      "Plot saved for sensor 3 with 607 data points\n",
      "Saved JARE4_data.csv with 1743 rows\n",
      "Plot saved for sensor 4 with 1679 data points\n",
      "Saved JARE5_data.csv with 898 rows\n",
      "Plot saved for sensor 5 with 594 data points\n",
      "Data processing completed.\n",
      "Script completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from netCDF4 import Dataset, num2date\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "dataset = Dataset('jare.nc')\n",
    "print(dataset)  # Prints summary info\n",
    "print(dataset.variables.keys())  # Prints variable names\n",
    "print(dataset.variables['time'])\n",
    "\n",
    "def process_wave_data(file_path=\"jare.nc\"):\n",
    "    try:\n",
    "        # Load netCDF file\n",
    "        a = Dataset(file_path, 'r')\n",
    "        \n",
    "        # Read variables\n",
    "        buoy = a.variables['buoy'][:]\n",
    "        lat = a.variables['lat'][:]\n",
    "        lon = a.variables['lon'][:]\n",
    "        psd = a.variables['psd'][:]\n",
    "        fbin = a.variables['bin'][:]  # Changed from bin_\n",
    "        deployed = a.variables['deployed'][:]\n",
    "        moment = a.variables['moments_processed'][:]\n",
    "        time = a.variables['time'][:]  # Added [:] to actually read the data\n",
    "        time_units = \"seconds since 2019-01-01 00:00:00 UTC\"\n",
    "        time_calendar = 'gregorian'\n",
    "\n",
    "        # Create boolean masks\n",
    "        lat_zero_mask = (lat == 0)  \n",
    "        deployed_mask = (deployed != 1)\n",
    "\n",
    "        # Apply masks to lat\n",
    "        lat_mask = lat_zero_mask | deployed_mask\n",
    "        lat_masked = np.ma.masked_array(lat, mask=lat_mask)\n",
    "        \n",
    "        # Create masked arrays\n",
    "        lat_masked = np.ma.masked_array(lat, mask=lat_mask)\n",
    "        lon_masked = np.ma.masked_array(lon, mask=lat_mask)\n",
    "        lat_mask_expanded = np.expand_dims(lat_mask, axis=2)\n",
    "        psd_masked = np.ma.masked_array(psd, mask=np.broadcast_to(lat_mask_expanded, psd.shape))\n",
    "        moment_masked = np.ma.masked_array(moment, mask=np.broadcast_to(lat_mask_expanded, moment.shape))\n",
    "\n",
    "        # Convert to datetime objects\n",
    "        dates = num2date(time, units=time_units, calendar=time_calendar)\n",
    "        # Convert to strings maintaining the same length\n",
    "        date_strings = []\n",
    "        for d in dates:\n",
    "            try:\n",
    "                if np.ma.is_masked(d):\n",
    "                    date_strings.append(None)  # or use np.nan\n",
    "                else:\n",
    "                    date_strings.append(pd.Timestamp(d.isoformat()).strftime(\"%d/%m/%Y %H:%M (UTC)\"))\n",
    "            except:\n",
    "                date_strings.append(None)  # or use np.nan\n",
    "                \n",
    "        # Calculate significant wave height\n",
    "        Hs = 4 * np.sqrt(moment_masked[:, :, 2])\n",
    "        \n",
    "        # Calculate peak period\n",
    "        Tp = np.full((5, 20448), np.nan)\n",
    "        for i in range(5):\n",
    "            for j in range(20448):\n",
    "                if not ma.is_masked(psd[i,j,0]):\n",
    "                    if ma.is_masked(psd_masked[i,j,:]):\n",
    "                        psd_slice = psd_masked[i,j,:].compressed()\n",
    "                        if len(psd_slice) > 0:\n",
    "                            mi = np.argmax(psd_slice)\n",
    "                            Tp[i,j] = 1/fbin[mi]  # Changed from bin_ to fbin\n",
    "                    else:\n",
    "                        mi = np.argmax(psd_masked[i,j,:])\n",
    "                        Tp[i,j] = 1/fbin[mi]\n",
    "                \n",
    "                if (not ma.is_masked(Hs[i,j]) and Hs[i,j] < 0.1):\n",
    "                    Hs[i,j] = np.nan\n",
    "                    Tp[i,j] = np.nan\n",
    "\n",
    "        # Process each sensor\n",
    "        for sensor_idx in range(5):  \n",
    "            # Create data for DataFrame\n",
    "            data = {\n",
    "                'DD/MM/YYYY HH:MM (UTC)': date_strings,\n",
    "                'Latitude (decimal degrees)': lat_masked[sensor_idx, :],\n",
    "                'Longitude (decimal degrees)': lon_masked[sensor_idx, :],\n",
    "                'Significant Wave Height (m)': Hs[sensor_idx, :],\n",
    "                'Peak Period (s)': Tp[sensor_idx, :]\n",
    "            }\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Filter out rows with missing lat/lon\n",
    "            df_filtered = df.dropna(subset=['Latitude (decimal degrees)', 'Longitude (decimal degrees)'])\n",
    "            \n",
    "            # Save to CSV\n",
    "            csv_filename = f'JARE{sensor_idx+1}_data.csv'  # Use sensor_idx instead of original_sensor_id\n",
    "            df_filtered.to_csv(csv_filename, index=False)\n",
    "            print(f\"Saved {csv_filename} with {len(df_filtered)} rows\")\n",
    "               \n",
    "            try:\n",
    "                # Plot data\n",
    "                if len(df_filtered) > 0:\n",
    "                    create_plots(df_filtered, sensor_idx+1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating plot for sensor {sensor_idx+1}: {str(e)}\")\n",
    "                \n",
    "\n",
    "        # Close the netCDF file\n",
    "        a.close()  # Changed from nc to a\n",
    "        print(\"Data processing completed.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def create_plots(df, sensor_id):\n",
    "    try:\n",
    "        # Create a fresh DataFrame to avoid the SettingWithCopyWarning\n",
    "        plot_df = df[['DD/MM/YYYY HH:MM (UTC)', 'Significant Wave Height (m)', 'Peak Period (s)']].copy()\n",
    "        plot_df = plot_df.dropna()\n",
    "        \n",
    "        if len(plot_df) == 0:\n",
    "            print(f\"No valid data to plot for sensor {sensor_id}\")\n",
    "            return\n",
    "\n",
    "        # Remove the \" (UTC)\" suffix and convert to datetime\n",
    "        plot_df['datetime'] = pd.to_datetime(\n",
    "            plot_df['DD/MM/YYYY HH:MM (UTC)'].str.replace(' (UTC)', ''),\n",
    "            format='%d/%m/%Y %H:%M'\n",
    "        )\n",
    "        \n",
    "        # Sort by date for chronological plotting\n",
    "        plot_df = plot_df.sort_values('datetime')\n",
    "        \n",
    "        # Calculate time span\n",
    "        time_span = plot_df['datetime'].max() - plot_df['datetime'].min()\n",
    "        \n",
    "        # Create figure and primary axis for wave height\n",
    "        fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "        \n",
    "        # Plot wave height\n",
    "        color1 = 'tab:blue'\n",
    "        ax1.set_ylabel('Significant Wave Height (m)', color=color1, fontsize=12)\n",
    "        line1 = ax1.plot(plot_df['datetime'], plot_df['Significant Wave Height (m)'],\n",
    "                        color=color1, label='Significant Wave Height',\n",
    "                        linestyle='None', marker='o', markersize=4)\n",
    "        ax1.tick_params(axis='y', labelcolor=color1)\n",
    "        \n",
    "        # Set y-axis limit for wave height\n",
    "        ax1.set_ylim(0, min(20, plot_df['Significant Wave Height (m)'].max() * 1.1))\n",
    "        \n",
    "        # Format x-axis based on time span\n",
    "        if time_span.days > 180:  # More than 6 months\n",
    "            date_format = mdates.DateFormatter('%b')\n",
    "            ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "            xlabel = f'Date/Time (UTC) - {plot_df[\"datetime\"].min().strftime(\"%Y\")}'\n",
    "        elif time_span.days > 30:  # More than a month\n",
    "            date_format = mdates.DateFormatter('%d %b')\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator(interval=7))  # Changed from WeekLocator\n",
    "            xlabel = f'Date/Time (UTC) - {plot_df[\"datetime\"].min().strftime(\"%Y\")}'\n",
    "        elif time_span.days > 7:  # More than a week\n",
    "            date_format = mdates.DateFormatter('%d %b')\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator())\n",
    "            xlabel = f'Date/Time (UTC) - {plot_df[\"datetime\"].min().strftime(\"%Y\")}'\n",
    "        elif time_span.days > 1:  # More than a day\n",
    "            date_format = mdates.DateFormatter('%d %H:%M')\n",
    "            ax1.xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "            xlabel = f'Date/Time (UTC) - {plot_df[\"datetime\"].min().strftime(\"%b %Y\")}'\n",
    "        else:  # Less than a day\n",
    "            date_format = mdates.DateFormatter('%H:%M')\n",
    "            ax1.xaxis.set_major_locator(mdates.HourLocator())\n",
    "            xlabel = f'Date/Time (UTC) - {plot_df[\"datetime\"].min().strftime(\"%d %b %Y\")}'\n",
    "        \n",
    "        ax1.xaxis.set_major_formatter(date_format)\n",
    "        \n",
    "        # Rotate and align the tick labels so they look better\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add title and legend\n",
    "        plt.title(f'Wave Data - Sensor {sensor_id}', fontsize=14)\n",
    "        ax1.legend(loc='upper right', fontsize=11)\n",
    "        \n",
    "        # Add grid and labels\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlabel(xlabel, fontsize=12)\n",
    "        \n",
    "        # Adjust layout and save\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(f'JARE{sensor_id}_plot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Plot saved for sensor {sensor_id} with {len(plot_df)} data points\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating plot for sensor {sensor_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Call the function to process the data\n",
    "if __name__ == \"__main__\":\n",
    "    success = process_wave_data()\n",
    "    \n",
    "    if success:\n",
    "        print(\"Script completed successfully.\")\n",
    "    else:\n",
    "        print(\"Script failed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13aa096-3793-47a7-ba6f-03226178d9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a2ebb-1c51-456c-bdc5-998b583b10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e3baa-2a0f-43c5-8266-dcdc7afe537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def64b6d-dd68-423c-82fe-a059eb3d1cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f7bec-324c-4965-b588-5b7a057c5ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed9422-953f-4297-b559-fe60ad3cd9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd50cc-f417-47fa-bb8d-1de041879661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9318c-f6e2-4e20-8919-4890b597b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
