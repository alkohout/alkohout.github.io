{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac8fce11-1528-4a97-add3-f5ab2a47464a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PIPERS_1_data.csv with 2316 rows\n",
      "Plot saved for sensor 1 with 979 data points\n",
      "Saved PIPERS_2_data.csv with 4225 rows\n",
      "Plot saved for sensor 2 with 1065 data points\n",
      "Saved PIPERS_3_data.csv with 378 rows\n",
      "Plot saved for sensor 3 with 357 data points\n",
      "Saved PIPERS_4_data.csv with 526 rows\n",
      "Plot saved for sensor 4 with 103 data points\n",
      "Saved PIPERS_5_data.csv with 2665 rows\n",
      "Plot saved for sensor 5 with 1733 data points\n",
      "Saved PIPERS_6_data.csv with 2335 rows\n",
      "Plot saved for sensor 6 with 1897 data points\n",
      "Saved PIPERS_7_data.csv with 3041 rows\n",
      "Plot saved for sensor 7 with 2475 data points\n",
      "Saved PIPERS_8_data.csv with 421 rows\n",
      "Plot saved for sensor 8 with 293 data points\n",
      "Saved PIPERS_9_data.csv with 1890 rows\n",
      "Plot saved for sensor 9 with 1863 data points\n",
      "Saved PIPERS_10_data.csv with 3133 rows\n",
      "Plot saved for sensor 10 with 2925 data points\n",
      "Saved PIPERS_11_data.csv with 378 rows\n",
      "Plot saved for sensor 11 with 305 data points\n",
      "Saved PIPERS_12_data.csv with 0 rows\n",
      "Saved PIPERS_13_data.csv with 289 rows\n",
      "Plot saved for sensor 13 with 55 data points\n",
      "Saved PIPERS_14_data.csv with 1869 rows\n",
      "Plot saved for sensor 14 with 552 data points\n",
      "Data processing completed.\n",
      "Script completed successfully.\n"
     ]
    }
   ],
   "source": [
    "#!pip install netCDF4\n",
    "#from netCDF4 import Dataset\n",
    "\n",
    "#dataset = Dataset('sipexII.nc')\n",
    "#print(dataset)  # Prints summary info\n",
    "#print(dataset.variables.keys())  # Prints variable names\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from netCDF4 import Dataset, num2date\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "def process_wave_data(file_path=\"PIPERS.nc\"):\n",
    "    try:\n",
    "        # Load netCDF file\n",
    "        a = Dataset(file_path, 'r')\n",
    "        \n",
    "        # Read variables\n",
    "        buoy = a.variables['buoy'][:]\n",
    "        lat = a.variables['lat'][:]\n",
    "        lon = a.variables['lon'][:]\n",
    "        psd = a.variables['psd'][:]\n",
    "        fbin = a.variables['bin'][:]  # Changed from bin_\n",
    "        moment = a.variables['moments'][:]\n",
    "        time = a.variables['time'][:]  # Added [:] to actually read the data\n",
    "        time_units = \"seconds since 2017-01-01 00:00:00 UTC\"\n",
    "        time_calendar = 'gregorian'\n",
    "\n",
    "        # Create boolean masks\n",
    "        lat_zero_mask = (lat == 0)  \n",
    "        lat_mask = lat_zero_mask  # Added definition for lat_mask\n",
    "        \n",
    "        # Create masked arrays\n",
    "        lat_masked = np.ma.masked_array(lat, mask=lat_mask)\n",
    "        lon_masked = np.ma.masked_array(lon, mask=lat_mask)\n",
    "        lat_mask_expanded = np.expand_dims(lat_mask, axis=2)\n",
    "        psd_masked = np.ma.masked_array(psd, mask=np.broadcast_to(lat_mask_expanded, psd.shape))\n",
    "        moment_masked = np.ma.masked_array(moment, mask=np.broadcast_to(lat_mask_expanded, moment.shape))\n",
    "\n",
    "        # Convert to datetime objects\n",
    "        dates = num2date(time, units=time_units, calendar=time_calendar)\n",
    "        # Convert to strings maintaining the same length\n",
    "        date_strings = []\n",
    "        for d in dates:\n",
    "            try:\n",
    "                if np.ma.is_masked(d):\n",
    "                    date_strings.append(None)  # or use np.nan\n",
    "                else:\n",
    "                    date_strings.append(pd.Timestamp(d.isoformat()).strftime(\"%d/%m/%Y %H:%M (UTC)\"))\n",
    "            except:\n",
    "                date_strings.append(None)  # or use np.nan\n",
    "                \n",
    "        # Calculate significant wave height\n",
    "        Hs = 4 * np.sqrt(moment_masked[:, :, 2])\n",
    "        \n",
    "        # Calculate peak period\n",
    "        Tp = np.full((14, 12000), np.nan)\n",
    "        for i in range(14):\n",
    "            for j in range(12000):\n",
    "                if not ma.is_masked(psd[i,j,0]):\n",
    "                    if ma.is_masked(psd_masked[i,j,:]):\n",
    "                        psd_slice = psd_masked[i,j,:].compressed()\n",
    "                        if len(psd_slice) > 0:\n",
    "                            mi = np.argmax(psd_slice)\n",
    "                            Tp[i,j] = 1/fbin[mi]  # Changed from bin_ to fbin\n",
    "                    else:\n",
    "                        mi = np.argmax(psd_masked[i,j,:])\n",
    "                        Tp[i,j] = 1/fbin[mi]\n",
    "                \n",
    "                if (not ma.is_masked(Hs[i,j]) and Hs[i,j] < 0.1):\n",
    "                    Hs[i,j] = np.nan\n",
    "                    Tp[i,j] = np.nan\n",
    "\n",
    "        # Process each sensor\n",
    "        for sensor_idx in range(14):  # Changed from 15 to 14 to match array dimensions\n",
    "            # Create data for DataFrame\n",
    "            data = {\n",
    "                'DD/MM/YYYY HH:MM (UTC)': date_strings,\n",
    "                'Latitude (decimal degrees)': lat_masked[sensor_idx, :],\n",
    "                'Longitude (decimal degrees)': lon_masked[sensor_idx, :],\n",
    "                'Significant Wave Height (m)': Hs[sensor_idx, :],\n",
    "                'Peak Period (s)': Tp[sensor_idx, :]\n",
    "            }\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Filter out rows with missing lat/lon\n",
    "            df_filtered = df.dropna(subset=['Latitude (decimal degrees)', 'Longitude (decimal degrees)'])\n",
    "            \n",
    "            # Save to CSV\n",
    "            csv_filename = f'PIPERS{sensor_idx+1}_data.csv'  # Use sensor_idx instead of original_sensor_id\n",
    "            df_filtered.to_csv(csv_filename, index=False)\n",
    "            print(f\"Saved {csv_filename} with {len(df_filtered)} rows\")\n",
    "               \n",
    "            try:\n",
    "                # Plot data\n",
    "                if len(df_filtered) > 0:\n",
    "                    create_plots(df_filtered, sensor_idx+1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating plot for sensor {sensor_idx+1}: {str(e)}\")\n",
    "                \n",
    "\n",
    "        # Close the netCDF file\n",
    "        a.close()  # Changed from nc to a\n",
    "        print(\"Data processing completed.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def create_plots(df, sensor_id):\n",
    "    try:\n",
    "        import matplotlib.dates as mdates\n",
    "        \n",
    "        # Create a fresh DataFrame to avoid the SettingWithCopyWarning\n",
    "        plot_df = df[['DD/MM/YYYY HH:MM (UTC)', 'Significant Wave Height (m)', 'Peak Period (s)']].copy()\n",
    "        plot_df = plot_df.dropna()\n",
    "        \n",
    "        if len(plot_df) == 0:\n",
    "            print(f\"No valid dates for sensor {int(sensor_id)}\")\n",
    "            return\n",
    "\n",
    "        # Convert datetime strings to datetime objects\n",
    "        plot_df.loc[:, 'datetime'] = pd.to_datetime(plot_df['DD/MM/YYYY HH:MM (UTC)'], \n",
    "                                                   format='%d/%m/%Y %H:%M (UTC)')\n",
    "        \n",
    "        # Calculate time span using datetime objects\n",
    "        time_span = plot_df['datetime'].max() - plot_df['datetime'].min()\n",
    "        \n",
    "        # Create figure and primary axis for wave height\n",
    "        fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "        \n",
    "        # Plot wave height on the primary axis\n",
    "        color1 = 'tab:blue'\n",
    "        ax1.set_ylabel('Significant Wave Height (m)', color=color1, fontsize=12)\n",
    "        line1 = ax1.plot(plot_df['datetime'], plot_df['Significant Wave Height (m)'], \n",
    "                 color=color1, label='Significant Wave Height',linestyle='None', marker='o', markersize=4)\n",
    "        ax1.tick_params(axis='y', labelcolor=color1)\n",
    "        \n",
    "        # Set y-axis limit for wave height\n",
    "        ax1.set_ylim(0, min(20, plot_df['Significant Wave Height (m)'].max() * 1.1))\n",
    "        \n",
    "        # Create second y-axis\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        # Plot peak period on secondary axis\n",
    "        color2 = 'tab:red'\n",
    "        ax2.set_ylabel('Peak Period (s)', color=color2, fontsize=12)\n",
    "        line2 = ax2.plot(plot_df['datetime'], plot_df['Peak Period (s)'], \n",
    "                 color=color2, label='Peak Period', marker='x',linestyle='None')\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "        \n",
    "        # Format x-axis based on time span\n",
    "        if time_span.days > 180:\n",
    "            date_format = mdates.DateFormatter('%b %Y')\n",
    "            ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "        elif time_span.days > 30:\n",
    "            date_format = mdates.DateFormatter('%d %b')\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator(interval=7))  # Changed from WeekLocator\n",
    "        elif time_span.days > 7:\n",
    "            date_format = mdates.DateFormatter('%d %b')\n",
    "            ax1.xaxis.set_major_locator(mdates.DayLocator())\n",
    "        elif time_span.days > 1:\n",
    "            date_format = mdates.DateFormatter('%d %H:%M')\n",
    "            ax1.xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "        else:\n",
    "            date_format = mdates.DateFormatter('%H:%M')\n",
    "            ax1.xaxis.set_major_locator(mdates.HourLocator())\n",
    "            \n",
    "        ax1.xaxis.set_major_formatter(date_format)\n",
    "        \n",
    "        # Rotate and align the tick labels so they look better\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add title and legend\n",
    "        plt.title(f'Wave Data - Sensor {int(sensor_id)}', fontsize=14)\n",
    "        ax1.legend(line1 + line2, ['Significant Wave Height', 'Peak Period'], \n",
    "                  loc='upper right', fontsize=11)\n",
    "        \n",
    "        # Add grid and adjust layout\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_xlabel('Date/Time (UTC)', fontsize=12)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(f'PIPERS{int(sensor_id)}_plot.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Plot saved for sensor {int(sensor_id)} with {len(plot_df)} data points\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating plot: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Call the function to process the data\n",
    "if __name__ == \"__main__\":\n",
    "    success = process_wave_data()\n",
    "    \n",
    "    if success:\n",
    "        print(\"Script completed successfully.\")\n",
    "    else:\n",
    "        print(\"Script failed. Check error messages above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13aa096-3793-47a7-ba6f-03226178d9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a2ebb-1c51-456c-bdc5-998b583b10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e3baa-2a0f-43c5-8266-dcdc7afe537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def64b6d-dd68-423c-82fe-a059eb3d1cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f7bec-324c-4965-b588-5b7a057c5ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed9422-953f-4297-b559-fe60ad3cd9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd50cc-f417-47fa-bb8d-1de041879661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9318c-f6e2-4e20-8919-4890b597b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
